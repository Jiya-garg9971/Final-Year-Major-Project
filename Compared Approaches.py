# -*- coding: utf-8 -*-
"""major_work_to_be _submitted

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cyLWvpKme-wRt_7sf1BkO6Ly86g0syoN
"""

import keras
import seaborn as sns
import matplotlib.pyplot as plt
from keras.models import Sequential
import PIL
import os
import cv2
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from PIL import Image
from keras.layers import Conv2D,Flatten,Dense,Dropout,BatchNormalization,MaxPooling2D
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import EarlyStopping,ModelCheckpoint
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from tqdm import tqdm
from imblearn.over_sampling import SMOTE

from sklearn.preprocessing import StandardScaler

import os
os.makedirs('/root/.kaggle', exist_ok=True)
!cp kaggle.json /root/.kaggle/
os.chmod('/root/.kaggle/kaggle.json', 600)  # Set permissions

!kaggle datasets download -d ninadaithal/imagesoasis



import zipfile

with zipfile.ZipFile('imagesoasis.zip', 'r') as zip_ref:  # Replace with your zip file name
    zip_ref.extractall('/content/dataset')  # Specify your extraction directory

non_demented = []
very_mild_demented = []
mild_demented = []
moderate_demented = []

# Download per category
for dirname, _, filenames in os.walk('/content/dataset/Data/Non Demented'):
    for filename in filenames:
        non_demented.append(os.path.join(dirname, filename))

for dirname, _, filenames in os.walk('/content/dataset/Data/Very mild Dementia'):
    for filename in filenames:
        very_mild_demented.append(os.path.join(dirname, filename))

for dirname, _, filenames in os.walk('/content/dataset/Data/Mild Dementia'):
    for filename in filenames:
        mild_demented.append(os.path.join(dirname, filename))

for dirname, _, filenames in os.walk('/content/dataset/Data/Moderate Dementia'):
    for filename in filenames:
        moderate_demented.append(os.path.join(dirname, filename))

images = []
labels = []
for subfolder in tqdm(os.listdir('/content/dataset')):
    subfolder_path = os.path.join('/content/dataset', subfolder)
    for folder in os.listdir(subfolder_path):
        subfolder_path2=os.path.join(subfolder_path,folder)
        for image_filename in os.listdir(subfolder_path2):
            image_path = os.path.join(subfolder_path2, image_filename)
            images.append(image_path)
            labels.append(folder)
df = pd.DataFrame({'image': images, 'label': labels})
df

plt.figure(figsize=(4,4))
ax = sns.countplot(x=df.label,palette='Set1')
ax.set_xlabel("Class",fontsize=10)
ax.set_ylabel("Count",fontsize=10)
plt.title('The Number Of Samples For Each Class',fontsize=10)
plt.grid(False)
plt.xticks(rotation=35)
plt.show()

plt.figure(figsize=(10,12))
for n,i in enumerate(np.random.randint(0,len(df),20)):
    plt.subplot(5,5,n+1)
    img=cv2.imread(df.image[i])
    img=cv2.resize(img,(224,224))
    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    plt.imshow(img)
    plt.axis('off')
    plt.title(df.label[i],fontsize=12)

PIL.Image.open(str(mild_demented[0]))

print(len(non_demented))
print(len(mild_demented))
print(len(moderate_demented))
print(len(very_mild_demented))

non_demented = np.random.choice(non_demented, 488, replace=False)
mild_demented = np.random.choice(mild_demented, 488, replace=False)
very_mild_demented = np.random.choice(very_mild_demented, 488, replace=False)

print(len(non_demented))
print(len(mild_demented))
print(len(moderate_demented))
print(len(very_mild_demented))

encoder= OneHotEncoder()
encoder.fit([[0],[1],[2],[3]])

data = []
result = []
for s in non_demented:
    img = Image.open(s)
    img = img.resize((128,128))
    img = np.array(img)
    if(img.shape == (128,128,3)):
        data.append(np.array(img))
        result.append(encoder.transform([[0]]).toarray())

for s in mild_demented:
    img = Image.open(s)
    img = img.resize((128,128))
    img = np.array(img)
    if(img.shape == (128,128,3)):
        data.append(np.array(img))
        result.append(encoder.transform([[1]]).toarray())

for s in moderate_demented:
    img = Image.open(s)
    img = img.resize((128,128))
    img = np.array(img)
    if(img.shape == (128,128,3)):
        data.append(np.array(img))
        result.append(encoder.transform([[2]]).toarray())

for s in very_mild_demented:
    img = Image.open(s)
    img = img.resize((128,128))
    img = np.array(img)
    if(img.shape == (128,128,3)):
        data.append(np.array(img))
        result.append(encoder.transform([[3]]).toarray())

X=np.array(data)
x=X.shape
x

y=np.array(result)
y=y.reshape(x[0],4)
y=np.argmax(y, axis=1)

X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=42, shuffle = True)

X_train.shape

X_test.shape

"""# **CNN MODEL**"""

model=Sequential()

model.add(Conv2D(filters=32,kernel_size=2,padding='Same',input_shape = (128,128,3)))

model.add(Conv2D(filters=32,kernel_size=2,padding='Same',activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters=64,kernel_size=2,padding='Same',activation='relu'))
model.add(Conv2D(filters=64,kernel_size=2,padding='Same',activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters=128,kernel_size=2,padding='Same',activation='relu'))
model.add(Conv2D(filters=128,kernel_size=2,padding='Same',activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))
model.add(Dropout(0.25))


model.add(Flatten())
model.add(Dense(256,activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(4,activation='softmax'))

print(model.summary())

model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])

from keras.callbacks import EarlyStopping
early_stopping=EarlyStopping(
    monitor="val_loss",
    min_delta=0.00001,
    patience=20,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True,
    start_from_epoch=0,
)

history= model.fit(X_train,y_train, validation_split=0.2,epochs=10,callbacks=[early_stopping],batch_size=128)
model.evaluate(X_test,y_test)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.plot(history.history['accuracy'],label='train')
plt.plot(history.history['val_accuracy'],label='validation')
plt.legend()
plt.show()

plt.plot(history.history['loss'],label='train')
plt.plot(history.history['val_loss'],label='validation')
plt.legend()
plt.show()

class_name = {0: "Mild Dementia",1: "Moderate Dementia" , 2: "Non Demented", 3: "Very mild Dementia"}

test_scores = model.evaluate(X_test,y_test)
test_scores

predicted_test_labels = model.predict(X_test)
res = model.predict(X_test)
predicted_test_labels =  np.argmax(predicted_test_labels,axis= 1 )
predicted_test_labels[1]

ypred = []
for i in res:
  temp = np.argmax(i)
  ypred.append(temp)
ypred = np.array(ypred)

from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score

accuracy_score(y_test,ypred)

from sklearn.metrics import precision_score, recall_score, f1_score, log_loss

# Calculate Precision
precision = precision_score(y_test, ypred, average='weighted')  # Use 'weighted' for multi-class support
print(f"Precision: {precision:.2f}")

# Calculate Recall
recall = recall_score(y_test, ypred, average='weighted')
print(f"Recall: {recall:.2f}")

# Calculate F1-Score
f1 = f1_score(y_test, ypred, average='weighted')
print(f"F1-Score: {f1:.2f}")

cf = confusion_matrix(y_true = y_test ,y_pred =  ypred)
plt.figure(figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')

ax = sns.heatmap(cf, cmap='Blues', annot=True, fmt='d', xticklabels=class_name, yticklabels=class_name)

plt.title('Alzheimer\'s Disease Diagnosis')
plt.xlabel('Prediction')
plt.ylabel('Truth')
plt.show(ax)

x = ConfusionMatrixDisplay(cf)
x.plot()

print(classification_report(y_true = y_test ,y_pred =  ypred))

"""# **RANDOM FOREST CLASSIFIER**

RF WITH PCA
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to 1D array for classification
result_labels = [np.argmax(label) for label in result]
result_labels = np.array(result_labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat, result_labels, test_size=0.2, random_state=42)

# Apply PCA to reduce dimensionality
pca = PCA(n_components=200)  # Adjust n_components based on your dataset
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Initialize and train the Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=250, max_depth=25, random_state=42)
rf_model.fit(X_train_pca, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test_pca)

# Evaluate the model using accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Random Forest Classifier: {accuracy:.2f}")

# --- Plotting Section ---
# 1. Plot accuracy as a bar plot
plt.figure(figsize=(6, 4))
plt.bar(['Random Forest'], [accuracy], color='skyblue')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.ylim(0, 1)
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score, log_loss

# Calculate Precision
precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class support
print(f"Precision: {precision:.2f}")

# Calculate Recall
recall = recall_score(y_test, y_pred, average='weighted')
print(f"Recall: {recall:.2f}")

# Calculate F1-Score
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"F1-Score: {f1:.2f}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Display the confusion matrix as a heatmap
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot(cmap='Blues')

# Show the plot
plt.title("Confusion Matrix for Random Forest Classifier")
plt.show()

"""# **KNN**"""

import numpy as np
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array for classification
result_labels = [np.argmax(label) for label in result]
result_labels = np.array(result_labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat, result_labels, test_size=0.2, random_state=42)

# Apply PCA for dimensionality reduction
pca = PCA(n_components=100)  # Adjust n_components based on dataset and model performance
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Initialize and train the K-Nearest Neighbors Classifier
# Here, n_neighbors is set to 5 by default; adjust based on experimentation
knn_classifier = KNeighborsClassifier(n_neighbors=3)
knn_classifier.fit(X_train_pca, y_train)

# Make predictions on the test set
y_pred = knn_classifier.predict(X_test_pca)

# Evaluate the model using accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of K-Nearest Neighbors Classifier: {accuracy:.2f}")

from sklearn.metrics import precision_score, recall_score, f1_score, log_loss

# Calculate Precision
precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class support
print(f"Precision: {precision:.2f}")

# Calculate Recall
recall = recall_score(y_test, y_pred, average='weighted')
print(f"Recall: {recall:.2f}")

# Calculate F1-Score
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"F1-Score: {f1:.2f}")

"""KNN with standard scaler"""

import numpy as np
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from sklearn.preprocessing import StandardScaler
# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array for classification
result_labels = [np.argmax(label) for label in result]
result_labels = np.array(result_labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat, result_labels, test_size=0.2, random_state=42)


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Initialize and train the K-Nearest Neighbors Classifier
# Here, n_neighbors is set to 5 by default; adjust based on experimentation
knn_classifier = KNeighborsClassifier(n_neighbors=3)
knn_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = knn_classifier.predict(X_test)

# Evaluate the model using accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of K-Nearest Neighbors Classifier: {accuracy:.2f}")

from sklearn.metrics import precision_score, recall_score, f1_score, log_loss

# Calculate Precision
precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class support
print(f"Precision: {precision:.2f}")

# Calculate Recall
recall = recall_score(y_test, y_pred, average='weighted')
print(f"Recall: {recall:.2f}")

# Calculate F1-Score
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"F1-Score: {f1:.2f}")

"""# **LGBM**"""

import numpy as np
import lightgbm as lgb
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert labels to a 1D array for classification
result_labels = [np.argmax(label) for label in result]  # Example: 0 for non-demented, 1 for mild, etc.
result_labels = np.array(result_labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat, result_labels, test_size=0.2, random_state=42)

# Apply PCA to reduce dimensionality
pca = PCA(n_components=100)  # Adjust n_components based on the dataset size and performance
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Convert the data to LightGBM Dataset format
train_data = lgb.Dataset(X_train_pca, label=y_train)
test_data = lgb.Dataset(X_test_pca, label=y_test, reference=train_data)

# Set LightGBM parameters for multi-class classification
params = {
    'objective': 'multiclass',
    'num_class': 4,                # Number of classes
    'metric': 'multi_logloss',
    'boosting_type': 'gbdt',
    'learning_rate': 0.1,
    'num_leaves': 31,
    'max_depth': -1,
    'verbose': -1 # moved verbose_eval here and renamed it to verbose
}

# Define early stopping callback
early_stopping_callback = lgb.early_stopping(stopping_rounds=10, verbose=False)

# Train the LightGBM model, including the early stopping callback
# Removed the extra comma after callbacks=[early_stopping_callback]
lgb_model = lgb.train(params, train_data,
                      valid_sets=[test_data],
                      callbacks=[early_stopping_callback],
                      )

# Make predictions on the test set
y_pred_prob = lgb_model.predict(X_test_pca)
y_pred = np.argmax(y_pred_prob, axis=1)

# Evaluate the model using accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of LightGBM: {accuracy:.2f}")

from sklearn.metrics import precision_score, recall_score, f1_score, log_loss

# Calculate Precision
precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class support
print(f"Precision: {precision:.2f}")

# Calculate Recall
recall = recall_score(y_test, y_pred, average='weighted')
print(f"Recall: {recall:.2f}")

# Calculate F1-Score
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"F1-Score: {f1:.2f}")

"""LGBM with standard scaler"""

import numpy as np
import lightgbm as lgb
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert labels to a 1D array for classification
result_labels = [np.argmax(label) for label in result]  # Example: 0 for non-demented, 1 for mild, etc.
result_labels = np.array(result_labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat, result_labels, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Convert the data to LightGBM Dataset format
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

# Set LightGBM parameters for multi-class classification
params = {
    'objective': 'multiclass',
    'num_class': 4,                # Number of classes
    'metric': 'multi_logloss',
    'boosting_type': 'gbdt',
    'learning_rate': 0.1,
    'num_leaves': 31,
    'max_depth': -1,
    'verbose': -1 # moved verbose_eval here and renamed it to verbose
}

# Define early stopping callback
early_stopping_callback = lgb.early_stopping(stopping_rounds=10, verbose=False)

# Train the LightGBM model, including the early stopping callback
# Removed the extra comma after callbacks=[early_stopping_callback]
lgb_model = lgb.train(params, train_data,
                      valid_sets=[test_data],
                      callbacks=[early_stopping_callback],
                      )

# Make predictions on the test set
y_pred_prob = lgb_model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)

# Evaluate the model using accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of LightGBM: {accuracy:.2f}")

from sklearn.metrics import precision_score, recall_score, f1_score, log_loss

# Calculate Precision
precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class support
print(f"Precision: {precision:.2f}")

# Calculate Recall
recall = recall_score(y_test, y_pred, average='weighted')
print(f"Recall: {recall:.2f}")

# Calculate F1-Score
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"F1-Score: {f1:.2f}")

from sklearn.model_selection import cross_val_score
lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=4)
cv_scores = cross_val_score(lgb_model, X_train_pca, y_train, cv=5, scoring='accuracy')
print(f"Cross-validation scores: {cv_scores}")

"""# **XGBoost**"""

import numpy as np
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to 1D array for classification
result_labels = [np.argmax(label) for label in result]
result_labels = np.array(result_labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat, result_labels, test_size=0.2, random_state=42)

# Initialize and train the XGBoost classifier
xgb_model = XGBClassifier(objective="multi:softmax", num_class=4, eval_metric="mlogloss")
xgb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of XGBoost: {accuracy:.2f}")

from sklearn.metrics import precision_score, recall_score, f1_score, log_loss

# Calculate Precision
precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class support
print(f"Precision: {precision:.2f}")

# Calculate Recall
recall = recall_score(y_test, y_pred, average='weighted')
print(f"Recall: {recall:.2f}")

# Calculate F1-Score
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"F1-Score: {f1:.2f}")

"""# **GRADIENT BOOST**"""

import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming you have the dataset loaded with features 'data' and labels 'result'

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array for classification
result_labels = [np.argmax(label) for label in result]
result_labels = np.array(result_labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat, result_labels, test_size=0.2, random_state=42)

# Initialize and train the Gradient Boosting Classifier
gb_model = GradientBoostingClassifier(
    n_estimators=5,  # Number of boosting stages to perform
    learning_rate=0.1,  # Step size at each iteration
    max_depth=3,        # Maximum depth of each tree
    random_state=42
)
gb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = gb_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Gradient Boosting Classifier: {accuracy:.2f}")

import matplotlib.pyplot as plt

def plot_feature_importance(model, X, title="Feature Importance"):
    # Get feature importances from the model
    feature_importances = model.feature_importances_

    # Sort features by importance
    sorted_idx = np.argsort(feature_importances)[::-1]

    # Plot the feature importance
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_idx)), feature_importances[sorted_idx], align='center')
    plt.yticks(range(len(sorted_idx)), [f'Feature {i}' for i in sorted_idx])
    plt.title(title)
    plt.xlabel('Importance')
    plt.ylabel('Features')
    plt.show()

# Visualize feature importance
plot_feature_importance(gb_model, X_train, title="Gradient Boosting Feature Importance")

from sklearn.metrics import precision_score, recall_score, f1_score, log_loss

# Calculate Precision
precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class support
print(f"Precision: {precision:.2f}")

# Calculate Recall
recall = recall_score(y_test, y_pred, average='weighted')
print(f"Recall: {recall:.2f}")

# Calculate F1-Score
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"F1-Score: {f1:.2f}")

"""# **SVM**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Flatten the MRI images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array
result_labels = [np.argmax(label) for label in result]  # Extract class labels
result_labels = np.array(result_labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat, result_labels, test_size=0.2, random_state=42)

# Standardize the features to normalize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train the SVM model
svm_model = SVC(kernel='poly', C=10, gamma=10, degree=4)  # Polynomial kernel
svm_model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = svm_model.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of SVM: {accuracy:.2f}")

# Plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, title="Confusion Matrix"):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap='viridis', xticks_rotation=45)
    plt.title(title)
    plt.show()

plot_confusion_matrix(y_test, y_pred, title="SVM Confusion Matrix")

from sklearn.metrics import precision_score, recall_score, f1_score, log_loss

# Calculate Precision
precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class support
print(f"Precision: {precision:.2f}")

# Calculate Recall
recall = recall_score(y_test, y_pred, average='weighted')
print(f"Recall: {recall:.2f}")

# Calculate F1-Score
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"F1-Score: {f1:.2f}")

"""# **Genetic Algorithm**"""

!pip install deap scikit-learn

import numpy as np
import matplotlib.pyplot as plt
from deap import base, creator, tools, algorithms
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Prepare your MRI data (assuming MRI data is stored in `data` and labels in `result`)
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)
result_labels = [np.argmax(label) for label in result]  # Assuming one-hot encoded labels
result_labels = np.array(result_labels)

# Split and scale the data
X_train, X_test, y_train, y_test = train_test_split(data_flat, result_labels, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define bounds for SVM hyperparameters
C_range = (0.1, 100)
gamma_range = (0.0001, 10)
degree_range = (2, 5)

# Set up DEAP genetic algorithm components
creator.create("FitnessMulti", base.Fitness, weights=(1.0, 1.0, 1.0, 1.0))
creator.create("Individual", list, fitness=creator.FitnessMulti)

toolbox = base.Toolbox()
toolbox.register("attr_C", np.random.uniform, *C_range)
toolbox.register("attr_gamma", np.random.uniform, *gamma_range)
toolbox.register("attr_degree", np.random.randint, *degree_range)
toolbox.register("individual", tools.initCycle, creator.Individual,
                 (toolbox.attr_C, toolbox.attr_gamma, toolbox.attr_degree), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# Objective function: precision, recall, f1-score, accuracy, and sensitivity
def eval_svm(individual):
    C, gamma, degree = individual
    model = SVC(kernel='poly', C=C, gamma=gamma, degree=int(degree))
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)  # Sensitivity
    f1 = f1_score(y_test, y_pred, average='weighted')

    return accuracy, precision, recall, f1

toolbox.register("evaluate", eval_svm)
toolbox.register("mate", tools.cxBlend, alpha=0.5)

def mutate(individual):
    individual[0] = np.clip(individual[0] + np.random.normal(0, 10), C_range[0], C_range[1])
    individual[1] = np.clip(individual[1] + np.random.normal(0, 1), gamma_range[0], gamma_range[1])
    individual[2] = np.clip(individual[2] + np.random.randint(-1, 2), degree_range[0], degree_range[1])
    return individual,

toolbox.register("mutate", mutate)
toolbox.register("select", tools.selTournament, tournsize=3)

def genetic_algorithm():
    POP_SIZE = 5
    N_GEN = 5
    CX_PB = 0.5
    MUT_PB = 0.2

    population = toolbox.population(n=POP_SIZE)
    best_metrics = []

    for gen in range(N_GEN):
        offspring = algorithms.varAnd(population, toolbox, cxpb=CX_PB, mutpb=MUT_PB)
        fits = list(map(toolbox.evaluate, offspring))

        for fit, ind in zip(fits, offspring):
            ind.fitness.values = fit

        population = toolbox.select(offspring, k=len(population))
        top_individual = tools.selBest(population, k=1)[0]
        best_metrics.append(top_individual.fitness.values)

        print(f"Generation {gen + 1}, Best Accuracy: {top_individual.fitness.values[0]:.2f}, "
              f"Precision: {top_individual.fitness.values[1]:.2f}, Recall: {top_individual.fitness.values[2]:.2f}, "
              f"F1-score: {top_individual.fitness.values[3]:.2f}")

    return tools.selBest(population, k=1)[0], best_metrics

best_solution, metrics = genetic_algorithm()

print(f"Best parameters found: C={best_solution[0]:.2f}, gamma={best_solution[1]:.4f}, degree={int(best_solution[2])}")

import numpy as np
import matplotlib.pyplot as plt
from deap import base, creator, tools, algorithms
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Prepare your MRI data (assuming MRI data is stored in `data` and labels in `result`)
# Flatten the images if they are not already flattened
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)
result_labels = [np.argmax(label) for label in result]  # Assuming one-hot encoded labels
result_labels = np.array(result_labels)

# Split and scale the data
X_train, X_test, y_train, y_test = train_test_split(data_flat, result_labels, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define bounds for SVM hyperparameters
C_range = (0.1, 100)
gamma_range = (0.0001, 10)
degree_range = (2, 5)

# Set up DEAP genetic algorithm components
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
toolbox.register("attr_C", np.random.uniform, *C_range)
toolbox.register("attr_gamma", np.random.uniform, *gamma_range)
toolbox.register("attr_degree", np.random.randint, *degree_range)
toolbox.register("individual", tools.initCycle, creator.Individual,
                 (toolbox.attr_C, toolbox.attr_gamma, toolbox.attr_degree), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# Objective function: accuracy score
def eval_svm(individual):
    C, gamma, degree = individual
    model = SVC(kernel='poly', C=C, gamma=gamma, degree=int(degree))
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy,

toolbox.register("evaluate", eval_svm)
toolbox.register("mate", tools.cxBlend, alpha=0.5)

# Mutation function to ensure valid ranges
def mutate(individual):
    individual[0] = np.clip(individual[0] + np.random.normal(0, 10), C_range[0], C_range[1])
    individual[1] = np.clip(individual[1] + np.random.normal(0, 1), gamma_range[0], gamma_range[1])
    individual[2] = np.clip(individual[2] + np.random.randint(-1, 2), degree_range[0], degree_range[1])
    return individual,

toolbox.register("mutate", mutate)
toolbox.register("select", tools.selTournament, tournsize=3)

# Genetic algorithm function with visualization tracking
def genetic_algorithm():
    POP_SIZE = 5  # Population size
    N_GEN = 5     # Number of generations
    CX_PB = 0.5   # Crossover probability
    MUT_PB = 0.2  # Mutation probability

    population = toolbox.population(n=POP_SIZE)

    best_accuracies = []  # To track best accuracy for each generation
    param_history_C = []  # To track evolution of C values
    param_history_gamma = []  # To track evolution of gamma values
    param_history_degree = []  # To track evolution of degree values

    for gen in range(N_GEN):
        offspring = algorithms.varAnd(population, toolbox, cxpb=CX_PB, mutpb=MUT_PB)
        fits = map(toolbox.evaluate, offspring)

        for fit, ind in zip(fits, offspring):
            ind.fitness.values = fit

        population = toolbox.select(offspring, k=len(population))

        top_individual = tools.selBest(population, k=1)[0]
        best_accuracies.append(top_individual.fitness.values[0])

        # Track parameter evolution for visualization
        param_history_C.append([ind[0] for ind in population])
        param_history_gamma.append([ind[1] for ind in population])
        param_history_degree.append([ind[2] for ind in population])

        print(f"Generation {gen + 1}, Best Accuracy: {top_individual.fitness.values[0]:.2f}")

    return tools.selBest(population, k=1)[0], best_accuracies, param_history_C, param_history_gamma, param_history_degree

best_solution, accuracies, param_history_C, param_history_gamma, param_history_degree = genetic_algorithm()

print(f"Best parameters found: C={best_solution[0]:.2f}, gamma={best_solution[1]:.4f}, degree={int(best_solution[2])}")

# Visualization

# 1. Plot best accuracy over generations
plt.figure(figsize=(12, 6))

# Plot best accuracy over generations
plt.subplot(2, 2, 1)
plt.plot(accuracies, label="Best Accuracy")
plt.title('Best Accuracy Over Generations')
plt.xlabel('Generation')
plt.ylabel('Best Accuracy')
plt.legend()

# 2. Plot the evolution of the C values over generations
plt.subplot(2, 2, 2)
for i in range(len(param_history_C)):
    plt.plot(param_history_C[i], label=f'Gen {i + 1}', alpha=0.6)
plt.title('Evolution of C Parameter Over Generations')
plt.xlabel('Population Index')
plt.ylabel('C Value')
plt.legend()

# 3. Plot the evolution of gamma values over generations
plt.subplot(2, 2, 3)
for i in range(len(param_history_gamma)):
    plt.plot(param_history_gamma[i], label=f'Gen {i + 1}', alpha=0.6)
plt.title('Evolution of Gamma Parameter Over Generations')
plt.xlabel('Population Index')
plt.ylabel('Gamma Value')
plt.legend()

# 4. Plot the evolution of degree values over generations
plt.subplot(2, 2, 4)
for i in range(len(param_history_degree)):
    plt.plot(param_history_degree[i], label=f'Gen {i + 1}', alpha=0.6)
plt.title('Evolution of Degree Parameter Over Generations')
plt.xlabel('Population Index')
plt.ylabel('Degree Value')
plt.legend()

plt.tight_layout()
plt.show()

"""## **Metaheuristic Techniques**

# **Particle Swarm Optimisation**
"""

!pip install pyswarms

import numpy as np
import pyswarms as ps
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Prepare your data (as in your previous code)
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)
result_labels = [np.argmax(label) for label in result]
result_labels = np.array(result_labels)

X_train, X_test, y_train, y_test = train_test_split(data_flat, result_labels, test_size=0.2, random_state=42)

# Define bounds for SVM hyperparameters: [C, gamma, degree]
# Lower and upper bounds for each parameter
bounds = ([0.1, 0.0001, 2], [100, 10, 5])

# Objective function for PSO to minimize
def svm_accuracy(params):
    C, gamma, degree = params
    degree = int(degree)  # Degree should be an integer for polynomial kernel
    model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    # Return negative accuracy because PSO minimizes the objective function
    return -accuracy

# Wrapper function for evaluating each particle's performance
def objective_function(particles):
    accuracies = []
    for particle in particles:
        accuracy = svm_accuracy(particle)
        accuracies.append(accuracy)
    return np.array(accuracies)

# Set PSO parameters
options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}  # PSO hyperparameters: cognitive, social, and inertia weights

# Initialize optimizer
optimizer = ps.single.GlobalBestPSO(n_particles=20, dimensions=3, options=options, bounds=bounds)

# Perform optimizations
best_cost, best_params = optimizer.optimize(objective_function, iters=5)  # You can increase the number of iterations

# Output the best parameters found
print(f"Best parameters found: C={best_params[0]:.2f}, gamma={best_params[1]:.4f}, degree={int(best_params[2])}")
print(f"Best accuracy achieved: {-best_cost:.2f}")

"""with image

"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array
result_labels = [np.argmax(label) for label in result]  # Extract class labels
result_labels = np.array(result_labels)

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# Define the ACO parameters
n_ants = 10               # Number of ants
n_iterations = 4          # Number of iterations
evaporation_rate = 0.5    # Pheromone evaporation rate
alpha = 1.0               # Pheromone influence
beta = 2.0                # Heuristic influence

# Pheromone matrices for each parameter (C, gamma, degree)
pheromone_C = np.ones(10) * 0.1  # Initial pheromone levels for C
pheromone_gamma = np.ones(10) * 0.1  # Initial pheromone levels for gamma
pheromone_degree = np.ones(5) * 0.1  # Initial pheromone levels for degree

# Define possible values for C, gamma, and degree
C_values = np.logspace(-2, 2, 10)
gamma_values = np.logspace(-3, 1, 10)
degree_values = np.arange(1, 6)

def evaluate_model(C, gamma, degree):
    """Train SVM and return accuracy for given hyperparameters."""
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)
    return accuracy_score(y_test, y_pred)

# ACO Optimization loop
best_accuracy = 0
best_params = None

# Tracking variables for visualization
accuracy_history = []
pheromone_C_history = []
pheromone_gamma_history = []
pheromone_degree_history = []

for iteration in range(n_iterations):
    iteration_accuracy = 0  # Track accuracy for this iteration

    # Each ant selects values for C, gamma, and degree based on pheromone levels
    for ant in range(n_ants):
        # Select parameter values based on pheromone probabilities
        C = np.random.choice(C_values, p=pheromone_C / pheromone_C.sum())
        gamma = np.random.choice(gamma_values, p=pheromone_gamma / pheromone_gamma.sum())
        degree = np.random.choice(degree_values, p=pheromone_degree / pheromone_degree.sum())

        # Evaluate the model with selected parameters
        accuracy = evaluate_model(C, gamma, degree)
        iteration_accuracy += accuracy  # Sum for averaging later

        # Update best solution if current one is better
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_params = (C, gamma, degree)

        # Update pheromones based on accuracy
        pheromone_C[C_values == C] += accuracy
        pheromone_gamma[gamma_values == gamma] += accuracy
        pheromone_degree[degree_values == degree - 1] += accuracy

    # Average accuracy for the iteration
    iteration_accuracy /= n_ants
    accuracy_history.append(iteration_accuracy)

    # Store pheromone levels for visualization
    pheromone_C_history.append(pheromone_C.copy())
    pheromone_gamma_history.append(pheromone_gamma.copy())
    pheromone_degree_history.append(pheromone_degree.copy())

    # Evaporate pheromones
    pheromone_C *= (1 - evaporation_rate)
    pheromone_gamma *= (1 - evaporation_rate)
    pheromone_degree *= (1 - evaporation_rate)

# Output the best result
print(f"Best solution found: C={best_params[0]}, gamma={best_params[1]}, degree={best_params[2]}")
print(f"Best accuracy: {best_accuracy:.2f}")

# Visualization of the ACO process
plt.figure(figsize=(12, 6))

# 1. Plot accuracy over iterations
plt.subplot(1, 2, 1)
plt.plot(accuracy_history, label='Average Accuracy')
plt.title('ACO: Accuracy Progress Over Iterations')
plt.xlabel('Iterations')
plt.ylabel('Average Accuracy')
plt.legend()

# 2. Plot pheromone levels for each parameter
# Plot pheromone levels for C
plt.subplot(1, 2, 2)
for i, pheromone in enumerate(pheromone_C_history):
    plt.plot(C_values, pheromone, label=f'Iteration {i+1}' if i % 2 == 0 else '', color='blue', alpha=0.7)
plt.title('ACO: Pheromone Levels for C')
plt.xlabel('C Values')
plt.ylabel('Pheromone Level')
plt.legend()

plt.tight_layout()
plt.show()

"""# **Simulated Annealing**"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array
result_labels = [np.argmax(label) for label in result]  # Extract class labels
result_labels = np.array(result_labels)

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# Evaluate SVM model function
def evaluate_model(C, gamma, degree):
    """Train SVM and return accuracy for given hyperparameters."""
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)
    return accuracy_score(y_test, y_pred)

# Simulated Annealing
np.random.seed(42)
current_solution = [10, 10, 4]  # Initial hyperparameters: C, gamma, degree
best_solution = current_solution
best_accuracy = evaluate_model(*best_solution)
temperature = 100  # Initial temperature
cooling_rate = 0.9  # Cooling rate

# Tracking variables for visualization
accuracy_history = [best_accuracy]
temperature_history = [temperature]

for i in range(5):  # Number of iterations
    # Generate a neighboring solution by slightly adjusting hyperparameters
    new_solution = [
        current_solution[0] + np.random.uniform(-1, 1),  # Adjust C
        current_solution[1] + np.random.uniform(-1, 1),  # Adjust gamma
        int(current_solution[2] + np.random.choice([-1, 1]))  # Adjust degree
    ]
    new_solution[2] = max(1, new_solution[2])  # Ensure degree >= 1

    # Evaluate new solution
    new_accuracy = evaluate_model(*new_solution)

    # Acceptance probability calculation
    if new_accuracy > best_accuracy:
        best_solution, best_accuracy = new_solution, new_accuracy
        current_solution = new_solution  # Accept better solution
    else:
        # Accept worse solution with a probability depending on temperature
        if np.random.rand() < np.exp((new_accuracy - best_accuracy) / temperature):
            current_solution = new_solution

    # Reduce temperature
    temperature *= cooling_rate

    # Store history for visualization
    accuracy_history.append(best_accuracy)
    temperature_history.append(temperature)

# Output the best result
print(f"Best solution found: C={best_solution[0]}, gamma={best_solution[1]}, degree={best_solution[2]}")
print(f"Best accuracy: {best_accuracy:.2f}")

# Visualizations
# 1. Plot accuracy over iterations
plt.figure(figsize=(10, 6))
plt.plot(accuracy_history, label='Accuracy', color='blue')
plt.title('Simulated Annealing: Accuracy Progress')
plt.xlabel('Iterations')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# 2. Plot temperature decay
plt.figure(figsize=(10, 6))
plt.plot(temperature_history, label='Temperature', color='red')
plt.title('Simulated Annealing: Temperature Decay')
plt.xlabel('Iterations')
plt.ylabel('Temperature')
plt.legend()
plt.grid(True)

plt.show()

"""# **Flower Pollination Optimisation**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array
result_labels = [np.argmax(label) for label in result]  # Extract class labels
result_labels = np.array(result_labels)

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# Define the FPA parameters
n_flowers = 10           # Number of flowers (solutions)
n_iterations = 3         # Number of iterations
switch_probability = 0.8 # Probability of switching between global and local pollination

# Define possible values for C, gamma, and degree
C_values = np.logspace(-2, 2, 10)
gamma_values = np.logspace(-3, 1, 10)
degree_values = np.arange(1, 6)

# Initialize flower solutions randomly within the parameter ranges
flowers = np.array([
    [np.random.choice(C_values),
     np.random.choice(gamma_values),
     np.random.choice(degree_values)]
    for _ in range(n_flowers)
])

def evaluate_model(C, gamma, degree):
    """Train SVM and return accuracy for given hyperparameters."""
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)
    return accuracy_score(y_test, y_pred)

# FPA Optimization loop
best_accuracy = 0
best_params = None

for iteration in range(n_iterations):
    for i in range(n_flowers):
        if np.random.rand() < switch_probability:  # Global pollination
            # Global pollination (Lévy flight inspired)
            step = np.random.normal(0, 1, size=3)  # Simple random walk
            new_solution = flowers[i] + step * (flowers[i] - flowers[np.argmax([evaluate_model(*f) for f in flowers])])
        else:  # Local pollination
            # Local pollination (randomly combine two flowers)
            j, k = np.random.choice(range(n_flowers), size=2, replace=False)
            new_solution = flowers[i] + np.random.rand() * (flowers[j] - flowers[k])

        # Clip new_solution to stay within the parameter ranges
        new_solution[0] = np.clip(new_solution[0], C_values.min(), C_values.max())  # C
        new_solution[1] = np.clip(new_solution[1], gamma_values.min(), gamma_values.max())  # gamma
        new_solution[2] = np.clip(new_solution[2], degree_values.min(), degree_values.max())  # degree

        # Evaluate new solution
        new_fitness = evaluate_model(new_solution[0], new_solution[1], new_solution[2])

        # Update flower if the new solution is better
        if new_fitness > best_accuracy:
            best_accuracy = new_fitness
            best_params = (new_solution[0], new_solution[1], new_solution[2])

        # Update flower solution if new solution is better
        flowers[i] = new_solution

    print(f"Iteration {iteration + 1}: Best Accuracy = {best_accuracy:.2f}")

print(f"Best solution found: C={best_params[0]}, gamma={best_params[1]}, degree={best_params[2]}")
print(f"Best accuracy: {best_accuracy:.2f}")

"""with visualisation

"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array
result_labels = [np.argmax(label) for label in result]  # Extract class labels
result_labels = np.array(result_labels)

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# Define the FPA parameters
n_flowers = 10           # Number of flowers (solutions)
n_iterations = 10        # Number of iterations
switch_probability = 0.8 # Probability of switching between global and local pollination

# Define possible values for C, gamma, and degree
C_values = np.logspace(-2, 2, 10)
gamma_values = np.logspace(-3, 1, 10)
degree_values = np.arange(1, 6)

# Initialize flower solutions randomly within the parameter ranges
flowers = np.array([
    [np.random.choice(C_values),
     np.random.choice(gamma_values),
     np.random.choice(degree_values)]
    for _ in range(n_flowers)
])

def evaluate_model(C, gamma, degree):
    """Train SVM and return accuracy for given hyperparameters."""
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=int(degree))
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)
    return accuracy_score(y_test, y_pred)

# FPA Optimization loop
best_accuracy = 0
best_params = None
best_accuracies = []  # Track best accuracy at each iteration
flower_positions = []  # Track flower positions

for iteration in range(n_iterations):
    flower_positions.append(flowers.copy())  # Record flower positions
    for i in range(n_flowers):
        if np.random.rand() < switch_probability:  # Global pollination
            # Global pollination (Lévy flight inspired)
            step = np.random.normal(0, 1, size=3)  # Simple random walk
            new_solution = flowers[i] + step * (flowers[i] - flowers[np.argmax([evaluate_model(*f) for f in flowers])])
        else:  # Local pollination
            # Local pollination (randomly combine two flowers)
            j, k = np.random.choice(range(n_flowers), size=2, replace=False)
            new_solution = flowers[i] + np.random.rand() * (flowers[j] - flowers[k])

        # Clip new_solution to stay within the parameter ranges
        new_solution[0] = np.clip(new_solution[0], C_values.min(), C_values.max())  # C
        new_solution[1] = np.clip(new_solution[1], gamma_values.min(), gamma_values.max())  # gamma
        new_solution[2] = np.clip(new_solution[2], degree_values.min(), degree_values.max())  # degree

        # Evaluate new solution
        new_fitness = evaluate_model(new_solution[0], new_solution[1], new_solution[2])

        # Update flower if the new solution is better
        if new_fitness > best_accuracy:
            best_accuracy = new_fitness
            best_params = (new_solution[0], new_solution[1], new_solution[2])

        # Update flower solution if new solution is better
        flowers[i] = new_solution

    best_accuracies.append(best_accuracy)
    print(f"Iteration {iteration + 1}: Best Accuracy = {best_accuracy:.2f}")

print(f"Best solution found: C={best_params[0]}, gamma={best_params[1]}, degree={best_params[2]}")
print(f"Best accuracy: {best_accuracy:.2f}")

# Visualization of FPA Optimization

# 1. Plot the convergence of accuracy
plt.figure(figsize=(10, 6))
plt.plot(best_accuracies, marker='o', label="Best Accuracy")
plt.title("FPA Optimization: Convergence of Best Accuracy")
plt.xlabel("Iterations")
plt.ylabel("Accuracy")
plt.grid()
plt.legend()
plt.show()

# 2. Visualize flower positions in the search space
flower_positions = np.array(flower_positions)

fig = plt.figure(figsize=(12, 6))
ax = fig.add_subplot(111, projection='3d')

for i in range(n_flowers):
    ax.plot(
        flower_positions[:, i, 0],  # C positions
        flower_positions[:, i, 1],  # Gamma positions
        flower_positions[:, i, 2],  # Degree positions
        marker='o', markersize=2, alpha=0.7
    )

ax.set_title("Flower Positions in the Search Space")
ax.set_xlabel("C (log scale)")
ax.set_ylabel("Gamma (log scale)")
ax.set_zlabel("Degree")
plt.show()

"""# **Zoological Search Optimisation**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = np.array([img.flatten() for img in data])

# Convert result labels to a 1D array
result_labels = np.array([np.argmax(label) for label in result])

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# Define the ZSO parameters
n_agents = 10              # Number of agents (solutions)
n_iterations = 3           # Number of iterations
exploration_rate = 0.5     # Probability of global exploration
exploitation_rate = 0.5    # Probability of refining local solutions
migration_factor = 0.2     # Controls random movement intensity

# Define possible values for C, gamma, and degree
C_values = np.logspace(-2, 2, 10)
gamma_values = np.logspace(-3, 1, 10)
degree_values = np.arange(1, 6)

# Initialize agent solutions randomly
agents = np.array([
    [np.random.choice(C_values),
     np.random.choice(gamma_values),
     np.random.choice(degree_values)]
    for _ in range(n_agents)
])

def evaluate_model(C, gamma, degree):
    """Train SVM and return accuracy, precision, recall, and F1-score for given hyperparameters."""
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=int(degree))
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)

    # Compute accuracy
    accuracy = accuracy_score(y_test, y_pred)

    # Compute classification metrics
    report = classification_report(y_test, y_pred, output_dict=True)
    precision = report["weighted avg"]["precision"]
    recall = report["weighted avg"]["recall"]
    f1_score = report["weighted avg"]["f1-score"]

    return accuracy, precision, recall, f1_score

# Evaluate initial agent population
fitness = np.array([evaluate_model(*agent) for agent in agents])
best_agent_idx = np.argmax(fitness[:, 0])  # Select best based on accuracy
best_agent = agents[best_agent_idx]
best_accuracy, best_precision, best_recall, best_f1 = fitness[best_agent_idx]

# ZSO Optimization loop
for iteration in range(n_iterations):
    new_population = []

    for i in range(n_agents):
        if i == best_agent_idx:
            new_population.append(best_agent)  # Keep the best unchanged
            continue

        # Exploration Phase (Random movement)
        if np.random.rand() < exploration_rate:
            random_step = np.random.uniform(-migration_factor, migration_factor, size=3)
            new_solution = agents[i] + random_step

        # Exploitation Phase (Move towards best agent)
        elif np.random.rand() < exploitation_rate:
            new_solution = agents[i] + np.random.rand() * (best_agent - agents[i])

        # Adaptation Phase (Influence of other agents)
        else:
            neighbor_idx = np.random.choice(n_agents)  # Select a random neighbor
            new_solution = agents[i] + np.random.rand() * (agents[neighbor_idx] - agents[i])

        # Clip values within the search space
        new_solution[0] = np.clip(new_solution[0], C_values.min(), C_values.max())  # C
        new_solution[1] = np.clip(new_solution[1], gamma_values.min(), gamma_values.max())  # gamma
        new_solution[2] = np.clip(new_solution[2], degree_values.min(), degree_values.max())  # degree

        new_population.append(new_solution)

    # Evaluate new population
    new_population = np.array(new_population)
    new_fitness = np.array([evaluate_model(*agent) for agent in new_population])

    # Select the best solution
    new_best_idx = np.argmax(new_fitness[:, 0])
    if new_fitness[new_best_idx, 0] > best_accuracy:
        best_agent_idx = new_best_idx
        best_agent = new_population[new_best_idx]
        best_accuracy, best_precision, best_recall, best_f1 = new_fitness[new_best_idx]

    # Update agent population
    agents = new_population
    fitness = new_fitness

    print(f"Iteration {iteration + 1}: Best Accuracy = {best_accuracy:.4f}")

print("\nBest solution found:")
print(f"C = {best_agent[0]:.4f}, gamma = {best_agent[1]:.4f}, degree = {int(best_agent[2])}")
print(f"Best Accuracy: {best_accuracy:.4f}")
print(f"Precision: {best_precision:.4f}, Recall: {best_recall:.4f}, F1-Score: {best_f1:.4f}")

"""# **Whale Optimisation Algorithm (WOA)**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array
result_labels = [np.argmax(label) for label in result]  # Extract class labels
result_labels = np.array(result_labels)

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# Define the WOA parameters
n_whales = 10              # Number of whales
n_iterations = 1           # Number of iterations
a_max = 2.0                 # Maximum value for the 'a' parameter

# Define possible values for C, gamma, and degree
C_values = np.logspace(-2, 2, 10)
gamma_values = np.logspace(-3, 1, 10)
degree_values = np.arange(1, 6)

# Initialize the positions of whales
positions_C = np.random.choice(C_values, n_whales)
positions_gamma = np.random.choice(gamma_values, n_whales)
positions_degree = np.random.choice(degree_values, n_whales)

# Best solution initialization
best_accuracy = 0
best_params = None

# Objective function to evaluate SVM
def evaluate_model(C, gamma, degree):
    """Train SVM and return accuracy for given hyperparameters."""
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)
    return accuracy_score(y_test, y_pred)

# WOA optimization loop
for iteration in range(n_iterations):
    a = a_max - iteration * (a_max / n_iterations)  # Linearly decreasing parameter

    for i in range(n_whales):
        r = np.random.rand()
        A = 2 * a * r - a       # Compute the coefficient A
        C = 2 * r               # Compute the coefficient C
        l = np.random.uniform(-1, 1)  # Logarithmic spiral factor
        p = np.random.rand()    # Probability to choose exploitation or exploration

        # Evaluate the new solution
        accuracy = evaluate_model(positions_C[i], positions_gamma[i], positions_degree[i])

        # Update the best solution if necessary
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_params = (positions_C[i], positions_gamma[i], positions_degree[i])

        # Check if best_params is still None before using it
        if best_params is not None:
            # Update positions for gamma and degree similarly
            D_gamma = np.abs(C * best_params[1] - positions_gamma[i])
            positions_gamma[i] = best_params[1] - A * D_gamma
            D_degree = np.abs(C * best_params[2] - positions_degree[i])
            positions_degree[i] = best_params[2] - A * D_degree
        else:
            # If best_params is None (first iteration), skip the update
            pass

        # Exploitation phase
        if p < 0.5:
            if np.random.rand() < 0.5:
                # Shrinking encircling
                D = np.abs(C * best_params[0] - positions_C[i])
                positions_C[i] = best_params[0] - A * D
            else:
                # Spiral updating
                D = np.abs(best_params[0] - positions_C[i])
                positions_C[i] = D * np.exp(2 * l) * np.cos(2 * np.pi * l) + best_params[0]
        else:
            # Exploration phase
            random_whale_idx = np.random.randint(n_whales)
            D = np.abs(C * positions_C[random_whale_idx] - positions_C[i])
            positions_C[i] = positions_C[random_whale_idx] - A * D

        # Update positions for gamma and degree similarly
        D_gamma = np.abs(C * best_params[1] - positions_gamma[i])
        positions_gamma[i] = best_params[1] - A * D_gamma
        D_degree = np.abs(C * best_params[2] - positions_degree[i])
        positions_degree[i] = best_params[2] - A * D_degree

        # Enforce boundaries
        positions_C[i] = np.clip(positions_C[i], C_values.min(), C_values.max())
        positions_gamma[i] = np.clip(positions_gamma[i], gamma_values.min(), gamma_values.max())
        positions_degree[i] = np.clip(positions_degree[i], degree_values.min(), degree_values.max())

        # Evaluate the new solution
        accuracy = evaluate_model(positions_C[i], positions_gamma[i], positions_degree[i])

        # Update the best solution if necessary
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_params = (positions_C[i], positions_gamma[i], positions_degree[i])

    print(f"Iteration {iteration + 1}/{n_iterations}, Best Accuracy: {best_accuracy:.2f}")

print(f"Best solution found: C={best_params[0]}, gamma={best_params[1]}, degree={best_params[2]}")
print(f"Best accuracy: {best_accuracy:.2f}")

"""# **Differential Evolution**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array
result_labels = [np.argmax(label) for label in result]  # Extract class labels
result_labels = np.array(result_labels)

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# Define the DE parameters
population_size = 5  # Number of candidate solutions
n_generations = 2    # Number of generations
F = 0.8               # Differential weight (mutation factor)
CR = 0.7              # Crossover probability

# Define the search space
C_bounds = (0.01, 100)       # Bounds for C
gamma_bounds = (0.001, 10)   # Bounds for gamma
degree_bounds = (1, 5)       # Bounds for degree (integer)

# Initialize the population
population = np.zeros((population_size, 3))
population[:, 0] = np.random.uniform(*C_bounds, size=population_size)  # C values
population[:, 1] = np.random.uniform(*gamma_bounds, size=population_size)  # gamma values
population[:, 2] = np.random.randint(*degree_bounds, size=population_size)  # degree values

# Evaluate the SVM model
def evaluate_model(params):
    """Train SVM and return accuracy for given hyperparameters."""
    C, gamma, degree = params
    degree = int(degree)  # Ensure degree is an integer
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)
    return accuracy_score(y_test, y_pred)

# DE Optimization loop
best_solution = None
best_score = 0

for generation in range(n_generations):
    for i in range(population_size):
        # Mutation: Select three random individuals (different from i)
        indices = [idx for idx in range(population_size) if idx != i]
        a, b, c = population[np.random.choice(indices, 3, replace=False)]

        # Generate mutant vector
        mutant = a + F * (b - c)

        # Ensure bounds
        mutant[0] = np.clip(mutant[0], *C_bounds)
        mutant[1] = np.clip(mutant[1], *gamma_bounds)
        mutant[2] = np.clip(mutant[2], *degree_bounds)

        # Crossover
        trial = np.copy(population[i])
        for j in range(3):  # For each hyperparameter
            if np.random.rand() < CR:
                trial[j] = mutant[j]

        # Evaluate trial vector
        trial_score = evaluate_model(trial)
        current_score = evaluate_model(population[i])

        # Selection: Replace if trial is better
        if trial_score > current_score:
            population[i] = trial

            # Update best solution
            if trial_score > best_score:
                best_score = trial_score
                best_solution = trial

    print(f"Generation {generation + 1}/{n_generations}, Best Accuracy: {best_score:.2f}")

# Print the best solution
best_C, best_gamma, best_degree = best_solution
print(f"Best solution found: C={best_C}, gamma={best_gamma}, degree={int(best_degree)}")
print(f"Best accuracy: {best_score:.2f}")

"""# **Cuckoo Search**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array
result_labels = [np.argmax(label) for label in result]  # Extract class labels
result_labels = np.array(result_labels)

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# Define the CSA parameters
num_nests = 3      # Number of nests (candidate solutions)
max_generations = 3 # Number of generations
pa = 0.25            # Probability of abandoning a nest
alpha = 0.01         # Step size scaling factor

# Define the bounds for hyperparameters
C_bounds = (0.01, 100)       # Bounds for C
gamma_bounds = (0.001, 10)   # Bounds for gamma
degree_bounds = (1, 5)       # Bounds for degree (integer)

# Generate initial nests randomly
nests = np.zeros((num_nests, 3))
nests[:, 0] = np.random.uniform(*C_bounds, size=num_nests)        # Initialize C values
nests[:, 1] = np.random.uniform(*gamma_bounds, size=num_nests)    # Initialize gamma values
nests[:, 2] = np.random.randint(*degree_bounds, size=num_nests)   # Initialize degree values

# Fitness function: Evaluate the SVM model
def evaluate_model(params):
    C, gamma, degree = params
    degree = int(degree)  # Ensure degree is an integer
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)
    return 1 - accuracy_score(y_test, y_pred)  # Minimize 1 - accuracy

# Get fitness of all nests
fitness = np.array([evaluate_model(nest) for nest in nests])

# Cuckoo Search Algorithm loop
for generation in range(max_generations):
    for i in range(num_nests):
        # Generate a new solution (cuckoo's egg) by Lévy flight
        step = alpha * np.random.normal(size=3)
        new_nest = nests[i] + step

        # Ensure bounds
        new_nest[0] = np.clip(new_nest[0], *C_bounds)
        new_nest[1] = np.clip(new_nest[1], *gamma_bounds)
        new_nest[2] = np.clip(new_nest[2], *degree_bounds)

        # Evaluate the new solution
        new_fitness = evaluate_model(new_nest)

        # Replace if the new solution is better
        if new_fitness < fitness[i]:
            nests[i] = new_nest
            fitness[i] = new_fitness

    # Abandon some nests and replace them randomly
    num_abandoned = int(pa * num_nests)
    abandoned_indices = np.random.choice(num_nests, num_abandoned, replace=False)
    for idx in abandoned_indices:
        nests[idx, 0] = np.random.uniform(*C_bounds)
        nests[idx, 1] = np.random.uniform(*gamma_bounds)
        nests[idx, 2] = np.random.randint(*degree_bounds)

    # Evaluate the new nests
    fitness = np.array([evaluate_model(nest) for nest in nests])

    # Track the best solution
    best_index = np.argmin(fitness)
    best_solution = nests[best_index]
    best_score = 1 - fitness[best_index]

    print(f"Generation {generation + 1}/{max_generations}, Best Accuracy: {best_score:.2f}")

# Final best solution
best_C, best_gamma, best_degree = best_solution
print(f"Best solution found: C={best_C}, gamma={best_gamma}, degree={int(best_degree)}")
print(f"Best accuracy: {best_score:.2f}")

"""# **Fire Fighter Optimisation Algorithm**

**FFA WITH GRAPH**
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = np.array([img.flatten() for img in data])
result_labels = np.array([np.argmax(label) for label in result])  # Extract class labels

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# FFOA parameters
num_firefighters = 15
num_generations = 4
fire_spread_rate = 0.3
extinction_rate = 0.7
search_step = 0.05

# Hyperparameter bounds
C_bounds = (0.01, 100)
gamma_bounds = (0.001, 10)
degree_bounds = (1, 5)

# Initialize firefighters
firefighters = np.zeros((num_firefighters, 3))
firefighters[:, 0] = np.random.uniform(*C_bounds, size=num_firefighters)
firefighters[:, 1] = np.random.uniform(*gamma_bounds, size=num_firefighters)
firefighters[:, 2] = np.random.randint(*degree_bounds, size=num_firefighters)

# Store metrics
accuracy_history, precision_history, recall_history, f1_history = [], [], [], []
C_values, gamma_values, degree_values = [], [], []

def evaluate_model(params):
    C, gamma, degree = params
    degree = int(degree)
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

    return 1 - acc, acc, precision, recall, f1  # Minimize 1 - accuracy

# Initial fitness evaluation
fitness = np.zeros(num_firefighters)
for i in range(num_firefighters):
    fitness[i], acc, prec, rec, f1 = evaluate_model(firefighters[i])
    accuracy_history.append(acc)
    precision_history.append(prec)
    recall_history.append(rec)
    f1_history.append(f1)
    C_values.append(firefighters[i][0])
    gamma_values.append(firefighters[i][1])
    degree_values.append(firefighters[i][2])

# FFOA Optimization loop
for generation in range(num_generations):
    best_index = np.argmin(fitness)
    best_solution = firefighters[best_index]
    best_score = 1 - fitness[best_index]

    for i in range(num_firefighters):
        if i != best_index:
            step = search_step * np.random.uniform(-1, 1, size=3)
            firefighters[i] += step * fire_spread_rate
            firefighters[i] = np.clip(firefighters[i], [C_bounds[0], gamma_bounds[0], degree_bounds[0]], [C_bounds[1], gamma_bounds[1], degree_bounds[1]])
            fitness[i], acc, prec, rec, f1 = evaluate_model(firefighters[i])

    extinction_indices = np.random.choice(num_firefighters, int(num_firefighters * extinction_rate), replace=False)
    for idx in extinction_indices:
        if idx != best_index:
            firefighters[idx] = [np.random.uniform(*C_bounds), np.random.uniform(*gamma_bounds), np.random.randint(*degree_bounds)]
            fitness[idx], acc, prec, rec, f1 = evaluate_model(firefighters[idx])

    accuracy_history.append(best_score)
    precision_history.append(prec)
    recall_history.append(rec)
    f1_history.append(f1)
    C_values.append(best_solution[0])
    gamma_values.append(best_solution[1])
    degree_values.append(best_solution[2])

    print(f"Generation {generation + 1}/{num_generations}, Best Accuracy: {best_score:.2f}")

best_C, best_gamma, best_degree = best_solution
print(f"Best solution: C={best_C}, gamma={best_gamma}, degree={int(best_degree)}")
print(f"Best accuracy: {best_score:.2f}")

# Plot results
fig, axs = plt.subplots(2, 2, figsize=(12, 10))
axs[0, 0].plot(C_values, accuracy_history, marker='o')
axs[0, 0].set_title('Accuracy vs C')
axs[0, 1].plot(gamma_values, accuracy_history, marker='o')
axs[0, 1].set_title('Accuracy vs Gamma')
axs[1, 0].plot(degree_values, accuracy_history, marker='o')
axs[1, 0].set_title('Accuracy vs Degree')
axs[1, 1].plot(range(len(accuracy_history)), accuracy_history, label='Accuracy')
axs[1, 1].plot(range(len(precision_history)), precision_history, label='Precision')
axs[1, 1].plot(range(len(recall_history)), recall_history, label='Recall')
axs[1, 1].plot(range(len(f1_history)), f1_history, label='F1-Score')
axs[1, 1].legend()
axs[1, 1].set_title('Performance Metrics over Generations')
plt.tight_layout()
plt.show()

"""**FFO with Other result metrics**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array
result_labels = [np.argmax(label) for label in result]  # Extract class labels
result_labels = np.array(result_labels)

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# Define the FFOA parameters
num_firefighters = 15  # Number of firefighters (candidate solutions)
num_generations = 1   # Number of generations
fire_spread_rate = 0.3   # Rate at which fire spreads
extinction_rate = 0.7    # Rate at which fire is extinguished
search_step = 0.05       # Step size for exploration

# Define the bounds for hyperparameters
C_bounds = (0.01, 100)        # Bounds for C
gamma_bounds = (0.001, 10)    # Bounds for gamma
degree_bounds = (1, 5)        # Bounds for degree (integer)

# Initialize firefighter positions randomly
firefighters = np.zeros((num_firefighters, 3))
firefighters[:, 0] = np.random.uniform(*C_bounds, size=num_firefighters)        # Initialize C values
firefighters[:, 1] = np.random.uniform(*gamma_bounds, size=num_firefighters)    # Initialize gamma values
firefighters[:, 2] = np.random.randint(*degree_bounds, size=num_firefighters)   # Initialize degree values

# Fitness function: Evaluate the SVM model
def evaluate_model(params):
    C, gamma, degree = params
    degree = int(degree)  # Ensure degree is an integer
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

    return 1 - acc, acc, precision, recall, f1  # Minimize 1 - accuracy

# Get fitness of all firefighters
fitness_metrics = np.array([evaluate_model(firefighter) for firefighter in firefighters])
fitness = fitness_metrics[:, 0]  # Extract 1 - accuracy values

# FFOA Optimization loop
for generation in range(num_generations):
    # Identify the "fire source" (best solution so far)
    best_index = np.argmin(fitness)
    best_solution = firefighters[best_index]
    best_score, best_acc, best_precision, best_recall, best_f1 = fitness_metrics[best_index]

    # Spread firefighters based on the fire spread rate
    for i in range(num_firefighters):
        if i != best_index:  # Don't spread the best solution
            step = search_step * np.random.uniform(-1, 1, size=3)
            firefighters[i] += step * fire_spread_rate

            # Ensure bounds
            firefighters[i][0] = np.clip(firefighters[i][0], *C_bounds)
            firefighters[i][1] = np.clip(firefighters[i][1], *gamma_bounds)
            firefighters[i][2] = np.clip(firefighters[i][2], *degree_bounds)

            # Evaluate new position
            fitness_metrics[i] = evaluate_model(firefighters[i])
            fitness[i] = fitness_metrics[i][0]

    # Apply extinction effect
    extinction_indices = np.random.choice(num_firefighters, int(num_firefighters * extinction_rate), replace=False)
    for idx in extinction_indices:
        if idx != best_index:  # Don't modify the best solution
            firefighters[idx][0] = np.random.uniform(*C_bounds)
            firefighters[idx][1] = np.random.uniform(*gamma_bounds)
            firefighters[idx][2] = np.random.randint(*degree_bounds)

            # Evaluate new position
            fitness_metrics[idx] = evaluate_model(firefighters[idx])
            fitness[idx] = fitness_metrics[idx][0]

    # Print progress
    print(f"Generation {generation + 1}/{num_generations}, Best Accuracy: {best_acc:.2f}, Precision: {best_precision:.2f}, Recall: {best_recall:.2f}, F1-score: {best_f1:.2f}")

# Final best solution
best_C, best_gamma, best_degree = best_solution
print(f"Best solution found: C={best_C}, gamma={best_gamma}, degree={int(best_degree)}")
print(f"Best Accuracy: {best_acc:.2f}, Precision: {best_precision:.2f}, Recall: {best_recall:.2f}, F1-score: {best_f1:.2f}")

"""# **Simmulated Annealing**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array
result_labels = [np.argmax(label) for label in result]  # Extract class labels
result_labels = np.array(result_labels)

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

def evaluate_model(C, gamma, degree):
    """Train SVM and return accuracy, precision, recall, and F1-score."""
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=int(degree))
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

    return accuracy, precision, recall, f1

# Simulated Annealing
np.random.seed(42)
current_solution = [10, 10, 4]  # Initial hyperparameters: C, gamma, degree
best_solution = current_solution
best_metrics = evaluate_model(*best_solution)  # Stores (accuracy, precision, recall, f1)
temperature = 100  # Initial temperature
cooling_rate = 0.9  # Cooling rate

for i in range(5):  # Number of iterations
    # Generate a neighboring solution by slightly adjusting hyperparameters
    new_solution = [
        current_solution[0] + np.random.uniform(-1, 1),  # Adjust C
        current_solution[1] + np.random.uniform(-1, 1),  # Adjust gamma
        int(current_solution[2] + np.random.choice([-1, 1]))  # Adjust degree
    ]
    new_solution[2] = max(1, new_solution[2])  # Ensure degree >= 1

    # Evaluate new solution
    new_metrics = evaluate_model(*new_solution)

    # Acceptance probability calculation
    if new_metrics[0] > best_metrics[0]:  # Select based on accuracy
        best_solution, best_metrics = new_solution, new_metrics
        current_solution = new_solution  # Accept better solution
    else:
        # Accept worse solution with a probability depending on temperature
        if np.random.rand() < np.exp((new_metrics[0] - best_metrics[0]) / temperature):
            current_solution = new_solution

    # Reduce temperature
    temperature *= cooling_rate

    print(f"Iteration {i + 1}: "
          f"Accuracy: {new_metrics[0]:.4f}, "
          f"Precision: {new_metrics[1]:.4f}, "
          f"Recall: {new_metrics[2]:.4f}, "
          f"F1-score: {new_metrics[3]:.4f}")

# Print the best solution
best_C, best_gamma, best_degree = best_solution
print(f"\nBest solution found: C={best_C:.4f}, gamma={best_gamma:.4f}, degree={best_degree}")
print(f"Best Accuracy: {best_metrics[0]:.4f}")
print(f"Precision: {best_metrics[1]:.4f}")
print(f"Recall: {best_metrics[2]:.4f}")
print(f"F1-score: {best_metrics[3]:.4f}")

"""# **ACO**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array
result_labels = [np.argmax(label) for label in result]  # Extract class labels
result_labels = np.array(result_labels)

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# Define the ACO parameters
n_ants = 5               # Number of ants
n_iterations = 2         # Number of iterations
evaporation_rate = 0.5    # Pheromone evaporation rate
alpha = 1.0               # Pheromone influence
beta = 2.0                # Heuristic influence

# Pheromone matrices for each parameter (C, gamma, degree)
pheromone_C = np.ones(10) * 0.1  # Initial pheromone levels for C
pheromone_gamma = np.ones(10) * 0.1  # Initial pheromone levels for gamma
pheromone_degree = np.ones(5) * 0.1  # Initial pheromone levels for degree

# Define possible values for C, gamma, and degree
C_values = np.logspace(-2, 2, 10)
gamma_values = np.logspace(-3, 1, 10)
degree_values = np.arange(1, 6)

def evaluate_model(C, gamma, degree):
    """Train SVM and return accuracy, precision, recall, and F1-score."""
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=int(degree))
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

    return accuracy, precision, recall, f1

# ACO Optimization loop
best_metrics = (0, 0, 0, 0)  # (accuracy, precision, recall, f1-score)
best_params = None

for iteration in range(n_iterations):
    # Each ant selects values for C, gamma, and degree based on pheromone levels
    for ant in range(n_ants):
        # Select parameter values based on pheromone probabilities
        C = np.random.choice(C_values, p=pheromone_C / pheromone_C.sum())
        gamma = np.random.choice(gamma_values, p=pheromone_gamma / pheromone_gamma.sum())
        degree = np.random.choice(degree_values, p=pheromone_degree / pheromone_degree.sum())

        # Evaluate the model with selected parameters
        accuracy, precision, recall, f1 = evaluate_model(C, gamma, degree)

        # Update best solution if current one is better
        if accuracy > best_metrics[0]:  # Select based on accuracy
            best_metrics = (accuracy, precision, recall, f1)
            best_params = (C, gamma, degree)

        # Update pheromones based on accuracy
        pheromone_C[C_values == C] += accuracy
        pheromone_gamma[gamma_values == gamma] += accuracy
        pheromone_degree[degree_values == degree - 1] += accuracy

    # Evaporate pheromones
    pheromone_C *= (1 - evaporation_rate)
    pheromone_gamma *= (1 - evaporation_rate)
    pheromone_degree *= (1 - evaporation_rate)

    print(f"Iteration {iteration + 1}: "
          f"Accuracy: {best_metrics[0]:.4f}, "
          f"Precision: {best_metrics[1]:.4f}, "
          f"Recall: {best_metrics[2]:.4f}, "
          f"F1-score: {best_metrics[3]:.4f}")

# Print the best solution
best_C, best_gamma, best_degree = best_params
print(f"\nBest solution found: C={best_C:.4f}, gamma={best_gamma:.4f}, degree={best_degree}")
print(f"Best Accuracy: {best_metrics[0]:.4f}")
print(f"Precision: {best_metrics[1]:.4f}")
print(f"Recall: {best_metrics[2]:.4f}")
print(f"F1-score: {best_metrics[3]:.4f}")

"""# **BLACK EAGLE OPTIMISATION**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
import random

# Generate synthetic data (replace with your MRI dataset)
data, result = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# Flatten and preprocess data
scaler = StandardScaler()
data = scaler.fit_transform(data)

# Convert result labels to 1D array
result_labels = np.array(result)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data, result_labels, test_size=0.2, random_state=42)

# Define the objective function for Black Eagle Optimization
def objective_function(params):
    C, gamma, degree = params
    C = 10 ** C  # Transform to a larger range
    gamma = 10 ** gamma
    degree = int(degree)

    model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    return -np.mean(scores)  # Negative accuracy for minimization

# Black Eagle Optimization Algorithm
class BlackEagleOptimization:
    def __init__(self, objective_func, bounds, population_size=4, max_iter=3):
        self.objective_func = objective_func
        self.bounds = bounds
        self.dim = len(bounds)
        self.population_size = population_size
        self.max_iter = max_iter
        self.population = None
        self.fitness = None

    def initialize_population(self):
        self.population = [
            [random.uniform(bound[0], bound[1]) for bound in self.bounds]
            for _ in range(self.population_size)
        ]
        self.fitness = [self.objective_func(ind) for ind in self.population]

    def move_population(self):
        new_population = []
        best_idx = np.argmin(self.fitness)
        best_solution = self.population[best_idx]

        for ind in self.population:
            new_ind = []
            for d in range(self.dim):
                r1 = random.uniform(0, 1)
                r2 = random.uniform(0, 1)
                r3 = random.uniform(-1, 1)
                step = r1 * (best_solution[d] - ind[d]) + r2 * r3
                new_value = ind[d] + step
                new_value = np.clip(new_value, self.bounds[d][0], self.bounds[d][1])
                new_ind.append(new_value)
            new_population.append(new_ind)

        return new_population

    def optimize(self):
        self.initialize_population()
        for iteration in range(self.max_iter):
            new_population = self.move_population()
            new_fitness = [self.objective_func(ind) for ind in new_population]

            # Combine populations and select the best individuals
            combined_population = self.population + new_population
            combined_fitness = self.fitness + new_fitness

            sorted_indices = np.argsort(combined_fitness)
            self.population = [combined_population[i] for i in sorted_indices[:self.population_size]]
            self.fitness = [combined_fitness[i] for i in sorted_indices[:self.population_size]]

            best_idx = np.argmin(self.fitness)
            best_solution = self.population[best_idx]
            best_score = -self.fitness[best_idx]
            print(f"Iteration {iteration + 1}/{self.max_iter}, Best Fitness: {best_score:.4f}")

        return best_solution, best_score

# Define bounds for C, gamma, and degree
bounds = [(-2, 2), (-3, 1), (2, 5)]  # Example ranges: C [0.01, 100], gamma [0.001, 10], degree [2, 5]

# Run Black Eagle Optimization
beo = BlackEagleOptimization(objective_func=objective_function, bounds=bounds, population_size=4, max_iter=3)
best_params, best_score = beo.optimize()
print(f"Best Parameters: {best_params}, Best CV Accuracy: {best_score:.4f}")

# Train SVM with the best parameters
best_C = 10 ** best_params[0]
best_gamma = 10 ** best_params[1]
best_degree = int(best_params[2])

final_svm = SVC(kernel='poly', C=best_C, gamma=best_gamma, degree=best_degree)
final_svm.fit(X_train, y_train)

# Evaluate the final model
y_pred = final_svm.predict(X_test)

# Compute evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Test Accuracy: {accuracy:.4f}")
print(f"Test Precision: {precision:.4f}")
print(f"Test Recall: {recall:.4f}")
print(f"Test F1-Score: {f1:.4f}")

"""## **HSO**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
import random

# Generate synthetic data (replace with your MRI dataset)
data, result = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# Flatten and preprocess data
scaler = StandardScaler()
data = scaler.fit_transform(data)

# Convert result labels to 1D array
result_labels = np.array(result)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data, result_labels, test_size=0.2, random_state=42)

# Define the objective function for Hyperbolic Sine Optimizer
def objective_function(params):
    C, gamma, degree = params
    C = 10 ** C  # Transform to a larger range
    gamma = 10 ** gamma
    degree = int(degree)

    model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    return -np.mean(scores)  # Negative accuracy for minimization

# Hyperbolic Sine Optimizer Algorithm
class HyperbolicSineOptimizer:
    def __init__(self, objective_func, bounds, population_size=5, max_iter=3, alpha=1.5):
        self.objective_func = objective_func
        self.bounds = bounds
        self.dim = len(bounds)
        self.population_size = population_size
        self.max_iter = max_iter
        self.alpha = alpha  # Controls the influence of the hyperbolic sine function
        self.population = None
        self.fitness = None

    def initialize_population(self):
        self.population = [
            [random.uniform(bound[0], bound[1]) for bound in self.bounds]
            for _ in range(self.population_size)
        ]
        self.fitness = [self.objective_func(ind) for ind in self.population]

    def update_population(self):
        new_population = []
        best_idx = np.argmin(self.fitness)
        best_solution = self.population[best_idx]

        for ind in self.population:
            new_ind = []
            for d in range(self.dim):
                r = random.random()
                sinh_term = self.alpha * np.sinh(r * (best_solution[d] - ind[d]))
                new_value = ind[d] + sinh_term
                new_value = np.clip(new_value, self.bounds[d][0], self.bounds[d][1])
                new_ind.append(new_value)
            new_population.append(new_ind)

        return new_population

    def optimize(self):
        self.initialize_population()
        for iteration in range(self.max_iter):
            new_population = self.update_population()
            new_fitness = [self.objective_func(ind) for ind in new_population]

            # Combine populations and select the best individuals
            combined_population = self.population + new_population
            combined_fitness = self.fitness + new_fitness

            sorted_indices = np.argsort(combined_fitness)
            self.population = [combined_population[i] for i in sorted_indices[:self.population_size]]
            self.fitness = [combined_fitness[i] for i in sorted_indices[:self.population_size]]

            best_idx = np.argmin(self.fitness)
            best_solution = self.population[best_idx]
            best_score = -self.fitness[best_idx]
            print(f"Iteration {iteration + 1}/{self.max_iter}, Best Fitness: {best_score:.4f}")

        return best_solution, best_score

# Define bounds for C, gamma, and degree
bounds = [(-2, 2), (-3, 1), (2, 5)]  # Example ranges: C [0.01, 100], gamma [0.001, 10], degree [2, 5]

# Run Hyperbolic Sine Optimizer
hso = HyperbolicSineOptimizer(objective_func=objective_function, bounds=bounds, population_size=5, max_iter=3)
best_params, best_score = hso.optimize()
print(f"Best Parameters: {best_params}, Best CV Accuracy: {best_score:.4f}")

# Train SVM with the best parameters
best_C = 10 ** best_params[0]
best_gamma = 10 ** best_params[1]
best_degree = int(best_params[2])

final_svm = SVC(kernel='poly', C=best_C, gamma=best_gamma, degree=best_degree)
final_svm.fit(X_train, y_train)

# Evaluate the final model
y_pred = final_svm.predict(X_test)

# Compute evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Test Accuracy: {accuracy:.4f}")
print(f"Test Precision: {precision:.4f}")
print(f"Test Recall: {recall:.4f}")
print(f"Test F1-Score: {f1:.4f}")

"""# **Large Language Model Evolutionary Algorithm**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = [img.flatten() for img in data]
data_flat = np.array(data_flat)

# Convert result labels to a 1D array
result_labels = [np.argmax(label) for label in result]  # Extract class labels
result_labels = np.array(result_labels)

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# LLMEA Parameters
population_size = 5      # Number of solutions
num_generations = 1     # Number of generations
mutation_rate = 0.2      # Probability of mutation
crossover_rate = 0.8     # Probability of crossover

# Define the bounds for hyperparameters
C_bounds = (0.01, 100)        # Bounds for C
gamma_bounds = (0.001, 10)    # Bounds for gamma
degree_bounds = (1, 5)        # Bounds for degree (integer)

# Initialize the population randomly
population = np.zeros((population_size, 3))
population[:, 0] = np.random.uniform(*C_bounds, size=population_size)         # Random C values
population[:, 1] = np.random.uniform(*gamma_bounds, size=population_size)     # Random gamma values
population[:, 2] = np.random.randint(*degree_bounds, size=population_size)    # Random degree values

# Fitness functions
def evaluate_model(params):
    C, gamma, degree = params
    degree = int(degree)
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    return accuracy, precision, recall, f1

def complexity_objective(params):
    C, gamma, degree = params
    return np.log10(C) + np.log10(gamma) + degree

# Multi-objective optimization: minimize (1 - accuracy) and complexity
def evaluate_individual(params):
    accuracy, precision, recall, f1 = evaluate_model(params)
    complexity = complexity_objective(params)
    return [1 - accuracy, 1 - precision, 1 - recall, 1 - f1, complexity]

# Dominance function: Pareto dominance comparison
def dominates(ind1, ind2):
    return all(x <= y for x, y in zip(ind1, ind2)) and any(x < y for x, y in zip(ind1, ind2))

# LLMEA optimization loop
pareto_front = []
for generation in range(num_generations):
    # Evaluate fitness for each individual
    fitness = np.array([evaluate_individual(ind) for ind in population])

    # Rank individuals based on Pareto dominance
    pareto_front = []
    for i, f1 in enumerate(fitness):
        if not any(dominates(f2, f1) for j, f2 in enumerate(fitness) if i != j):
            pareto_front.append((population[i], f1))  # Add to Pareto front

    # Generate next generation
    new_population = []
    while len(new_population) < population_size:
        if np.random.rand() < crossover_rate:  # Crossover
            parent1, parent2 = population[np.random.choice(len(pareto_front), size=2, replace=False)]
            crossover_point = np.random.randint(1, len(parent1))
            child = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))
        else:  # Mutation
            parent = population[np.random.randint(len(pareto_front))]
            child = parent.copy()
            gene_index = np.random.randint(len(child))
            if gene_index == 0:  # Mutate C
                child[gene_index] = np.random.uniform(*C_bounds)
            elif gene_index == 1:  # Mutate gamma
                child[gene_index] = np.random.uniform(*gamma_bounds)
            elif gene_index == 2:  # Mutate degree
                child[gene_index] = np.random.randint(*degree_bounds)

        new_population.append(child)

    population = np.array(new_population)

    # Log the current Pareto front
    print(f"Generation {generation + 1}/{num_generations}")
    for sol, fit in pareto_front:
        print(f"Solution: C={sol[0]:.2f}, gamma={sol[1]:.2f}, degree={int(sol[2])}, Accuracy={1 - fit[0]:.4f}, Precision={1 - fit[1]:.4f}, Recall={1 - fit[2]:.4f}, F1-score={1 - fit[3]:.4f}, Complexity={fit[4]:.4f}")

# Select final Pareto-optimal solutions
pareto_solutions = [sol for sol, _ in pareto_front]
print(f"Final Pareto-optimal solutions: {pareto_solutions}")

"""# **Flower Pollination Algorithm**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images
data_flat = np.array([img.flatten() for img in data])

# Convert result labels to a 1D array
result_labels = np.array([np.argmax(label) for label in result])

# Scale the data
scaler = StandardScaler()
data_flat_scaled = scaler.fit_transform(data_flat)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_flat_scaled, result_labels, test_size=0.2, random_state=42)

# ------------------------------
# Flower Pollination Algorithm (FPA) Parameters
# ------------------------------
population_size = 10    # Number of flowers (solutions)
num_generations = 1   # Number of iterations
p_switch = 0.8         # Probability of local pollination

# Define the bounds for hyperparameters
C_bounds = (0.01, 100)
gamma_bounds = (0.001, 10)
degree_bounds = (1, 5)  # Must be integers

# ------------------------------
# Objective Function (SVM Training)
# ------------------------------
def evaluate_svm(C, gamma, degree):
    """Train SVM and return accuracy, precision, recall, and F1-score."""
    degree = int(degree)  # Ensure integer value for degree
    svm_model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)
    svm_model.fit(X_train, y_train)
    y_pred = svm_model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

    return accuracy, precision, recall, f1

# ------------------------------
# Initialize the Population
# ------------------------------
population = np.zeros((population_size, 3))
population[:, 0] = np.random.uniform(*C_bounds, size=population_size)        # Random C values
population[:, 1] = np.random.uniform(*gamma_bounds, size=population_size)    # Random gamma values
population[:, 2] = np.random.randint(*degree_bounds, size=population_size)   # Random degree values

# Evaluate fitness for each solution
fitness = np.array([evaluate_svm(*flower) for flower in population])

# ------------------------------
# Flower Pollination Algorithm Loop
# ------------------------------
for generation in range(num_generations):
    new_population = population.copy()

    # Global Pollination (Levy Flight)
    for i in range(population_size):
        if np.random.rand() < p_switch:
            best_idx = np.argmax(fitness[:, 0])  # Choose best flower based on accuracy
            best_flower = population[best_idx]
            levy_step = np.random.uniform(-1, 1, size=3)
            new_population[i] = population[i] + levy_step * (best_flower - population[i])
        else:
            # Local Pollination (Self-Pollination & Neighbor Influence)
            j, k = np.random.choice(population_size, size=2, replace=False)
            new_population[i] = population[i] + np.random.uniform() * (population[j] - population[k])

        # Ensure boundaries are respected
        new_population[i][0] = np.clip(new_population[i][0], *C_bounds)
        new_population[i][1] = np.clip(new_population[i][1], *gamma_bounds)
        new_population[i][2] = np.clip(int(new_population[i][2]), *degree_bounds)

    # Evaluate new population
    new_fitness = np.array([evaluate_svm(*flower) for flower in new_population])

    # Selection: Replace if new solution is better
    for i in range(population_size):
        if new_fitness[i][0] > fitness[i][0]:  # Select based on accuracy
            population[i] = new_population[i]
            fitness[i] = new_fitness[i]

    # Logging
    best_idx = np.argmax(fitness[:, 0])  # Best solution based on accuracy
    best_solution = population[best_idx]
    print(f"Generation {generation+1}/{num_generations} - Best C: {best_solution[0]:.2f}, gamma: {best_solution[1]:.4f}, degree: {int(best_solution[2])}, Accuracy: {fitness[best_idx][0]:.4f}, Precision: {fitness[best_idx][1]:.4f}, Recall: {fitness[best_idx][2]:.4f}, F1-score: {fitness[best_idx][3]:.4f}")

# ------------------------------
# Final Best Solution
# ------------------------------
best_idx = np.argmax(fitness[:, 0])  # Select best based on accuracy
best_C, best_gamma, best_degree = population[best_idx]
print(f"\nFinal Best Solution: C={best_C:.2f}, gamma={best_gamma:.4f}, degree={int(best_degree)}")
print(f"Final Metrics -> Accuracy: {fitness[best_idx][0]:.4f}, Precision: {fitness[best_idx][1]:.4f}, Recall: {fitness[best_idx][2]:.4f}, F1-score: {fitness[best_idx][3]:.4f}")